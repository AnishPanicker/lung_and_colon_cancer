{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54c2bb67",
   "metadata": {},
   "source": [
    "# IMPORTING THE LIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62549ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models,layers\n",
    "import matplotlib.pyplot as mlt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284736ed",
   "metadata": {},
   "source": [
    "# Loading the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75e28c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMGSIZE=224\n",
    "BATCHSIZE=128\n",
    "CHANELS=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09a9552a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15000 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "data1=tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"cancer_img\",\n",
    "    shuffle=True,\n",
    "    image_size=(IMGSIZE,IMGSIZE),\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7558293",
   "metadata": {},
   "source": [
    "# Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e29ee4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lung_aca', 'lung_n', 'lung_scc']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = data1.class_names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e44e5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c16572b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 224, 224, 3)\n",
      "[[[ 85.474495  49.47449  199.04591 ]\n",
      "  [ 81.86224   43.581635 192.42346 ]\n",
      "  [ 98.841835  59.897957 202.64285 ]\n",
      "  ...\n",
      "  [109.37239   77.4438   222.17351 ]\n",
      "  [ 87.43351   50.790684 204.71915 ]\n",
      "  [ 80.928535  37.000008 188.00002 ]]\n",
      "\n",
      " [[ 89.        54.       196.98979 ]\n",
      "  [ 87.42857   52.428574 198.42857 ]\n",
      "  [ 79.14285   41.142857 188.68878 ]\n",
      "  ...\n",
      "  [ 98.5458    65.61721  214.28056 ]\n",
      "  [ 73.99989   36.999897 194.64275 ]\n",
      "  [ 73.28056   29.214277 181.63776 ]]\n",
      "\n",
      " [[ 74.984695  43.556126 182.31122 ]\n",
      "  [ 74.571434  40.571426 186.61734 ]\n",
      "  [ 75.92347   39.92347  191.92348 ]\n",
      "  ...\n",
      "  [ 85.85183   52.923244 203.48457 ]\n",
      "  [ 66.45405   28.45405  190.79073 ]\n",
      "  [ 66.357124  21.214304 181.30104 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[201.65816  184.82648  238.96936 ]\n",
      "  [204.23982  184.04593  239.64287 ]\n",
      "  [205.57147  181.4337   243.648   ]\n",
      "  ...\n",
      "  [172.14816  117.3624   245.28589 ]\n",
      "  [164.9796   110.97462  238.28577 ]\n",
      "  [170.85718  120.77046  241.81638 ]]\n",
      "\n",
      " [[198.6479   169.79062  237.64786 ]\n",
      "  [213.48477  181.69888  245.84189 ]\n",
      "  [210.78561  173.95381  241.59169 ]\n",
      "  ...\n",
      "  [163.45398  114.16833  234.1886  ]\n",
      "  [175.08702  127.77089  241.78589 ]\n",
      "  [187.14288  146.5716   252.07135 ]]\n",
      "\n",
      " [[196.65814  155.04083  236.01529 ]\n",
      "  [201.86232  159.51028  232.14804 ]\n",
      "  [188.00005  142.21436  217.597   ]\n",
      "  ...\n",
      "  [162.03067  119.60208  228.38779 ]\n",
      "  [184.43878  147.43378  245.07648 ]\n",
      "  [187.14282  158.37749  243.66325 ]]]\n"
     ]
    }
   ],
   "source": [
    " for image_batch, label_batch in data1.take(1):\n",
    "        print(imagebatch.shape)\n",
    "        print(imagebatch[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd78915c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmlt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_batch, label_batch \u001b[38;5;129;01min\u001b[39;00m data1\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m12\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mlt' is not defined"
     ]
    }
   ],
   "source": [
    "mlt.figure(figsize=(20,10))\n",
    "for image_batch, label_batch in data1.take(1):\n",
    "    for i in range(12):\n",
    "        ax = mlt.subplot(3,4,i+1)\n",
    "        mlt.imshow(image_batch[i].numpy().astype('uint8'))\n",
    "        mlt.title(class_names[label_batch[i]])\n",
    "        mlt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17bbf5c",
   "metadata": {},
   "source": [
    "# Spliting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c189b514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37f38393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_div_tf(ds,train_split=0.74,val_split=0.13,test_split=0.13,shuffle=True,shuffle_size=1000):\n",
    "    ds_size=len(ds)\n",
    "    if shuffle:\n",
    "        ds=ds.shuffle(shuffle_size,seed=5)\n",
    "    train_size=int(train_split*ds_size)\n",
    "    val_size=int(val_split*ds_size)\n",
    "    \n",
    "    train_ds=ds.take(train_size)\n",
    "    val_ds=ds.skip(train_size).take(val_size)\n",
    "    test_ds=ds.skip(train_size).skip(val_size)\n",
    "    return train_ds,val_ds,test_ds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "726fbaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds,val_ds,test_ds=get_dataset_div_tf(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52ce7a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b742c729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8823b4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c91a3ec",
   "metadata": {},
   "source": [
    "## using gpu and cpu together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bd33061",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds=train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_ds=test_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_ds=val_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5947713",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d673a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_rescale=tf.keras.Sequential([\n",
    "    layers.experimental.preprocessing.Resizing(IMG_SIZE,IMG_SIZE),# to convert the supplied image acc. to the model \n",
    "    layers.experimental.preprocessing.Rescaling(1.0/255) # to cover the rgb\n",
    "    \n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "333124df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for slight variation like rotation,zoom etc\n",
    "data_augment=tf.keras.Sequential([\n",
    "    layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.2)\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b824c8a",
   "metadata": {},
   "source": [
    "# Buliding the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57221237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "input_shape = (BATCH_SIZE, IMG_SIZE, IMG_SIZE, CHANNELS)\n",
    "n_classes = len(class_names)\n",
    "\n",
    "\n",
    "model = models.Sequential([\n",
    "    resize_rescale,\n",
    "    data_augment,\n",
    "    \n",
    "    layers.Conv2D(32, kernel_size = (3,3), activation='relu', input_shape=input_shape),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(n_classes, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.build(input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d661e1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (128, 224, 224, 3)        0         \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 222, 222, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 111, 111, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 109, 109, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 54, 54, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 52, 52, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 26, 26, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 12, 12, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 10, 10, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 3, 3, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 1, 1, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 171,459\n",
      "Trainable params: 171,459\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fa4d3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b5569b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "87/87 [==============================] - 684s 7s/step - loss: 0.5077 - accuracy: 0.7624 - val_loss: 0.3927 - val_accuracy: 0.8516\n",
      "Epoch 2/15\n",
      "87/87 [==============================] - 569s 7s/step - loss: 0.3137 - accuracy: 0.8729 - val_loss: 0.2813 - val_accuracy: 0.8797\n",
      "Epoch 3/15\n",
      "87/87 [==============================] - 577s 7s/step - loss: 0.2603 - accuracy: 0.8949 - val_loss: 0.2525 - val_accuracy: 0.8880\n",
      "Epoch 4/15\n",
      " 3/87 [>.............................] - ETA: 9:00 - loss: 0.2117 - accuracy: 0.9010"
     ]
    }
   ],
   "source": [
    "history=model.fit(\n",
    "    train_ds,\n",
    "    epochs=15,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1,\n",
    "    validation_data=val_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82a71a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f7cf66",
   "metadata": {},
   "source": [
    "# Plotting the Accuracy and Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91382574",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c3adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4477c891",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00dbcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlt.figure(figsize=(8, 8))\n",
    "mlt.subplot(1, 2, 1)\n",
    "mlt.plot(range(15), acc, label='Training Accuracy')\n",
    "mlt.plot(range(15), val_acc, label='Validation Accuracy')\n",
    "mlt.legend(loc='lower right')\n",
    "mlt.title('Training and Validation Accuracy')\n",
    "\n",
    "mlt.subplot(1, 2, 2)\n",
    "mlt.plot(range(15), loss, label='Training Loss')\n",
    "mlt.plot(range(15), val_loss, label='Validation Loss')\n",
    "mlt.legend(loc='upper right')\n",
    "mlt.title('Training and Validation Loss')\n",
    "mlt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40da57b8",
   "metadata": {},
   "source": [
    "# function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26234448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, img):\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())\n",
    "    img_array = tf.expand_dims(img_array, 0)\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "\n",
    "    predicted_class = class_names[np.argmax(predictions[0])]\n",
    "    confidence = round(100 * (np.max(predictions[0])), 2)\n",
    "    return predicted_class, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74478c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlt.figure(figsize=(15, 15))\n",
    "for images, labels in test_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = mlt.subplot(3, 3, i + 1)\n",
    "        mlt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        \n",
    "        predicted_class, confidence = predict(model, images[i].numpy())\n",
    "        actual_class = class_names[labels[i]] \n",
    "        \n",
    "        mlt.title(f\"Actual: {actual_class},\\n Predicted: {predicted_class}.\\n Confidence: {confidence}%\")\n",
    "        \n",
    "        mlt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487258bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_names[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf3604",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5846ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at f\"../models/{model_No}\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m      2\u001b[0m model_No\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../models/\u001b[39;49m\u001b[38;5;132;43;01m{model_No}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\saving\\legacy\\save.py:227\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[1;32m--> 227\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[0;32m    228\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    229\u001b[0m         )\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[0;32m    232\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m    233\u001b[0m             filepath_str, \u001b[38;5;28mcompile\u001b[39m, options\n\u001b[0;32m    234\u001b[0m         )\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at f\"../models/{model_No}\""
     ]
    }
   ],
   "source": [
    "model_No='15_epoch_lung'\n",
    "model.save(f\"../models/{model_No}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deb01d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeff219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
